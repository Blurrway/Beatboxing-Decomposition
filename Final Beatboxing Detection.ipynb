{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E190AP Final Project - Automatic Beatboxing Recognition\n",
    "### Team: Ankoor Apte and Sidney Cozier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Background and motivation: \n",
    "\n",
    "Beatboxing can be defined as the art of vocal percussion, or mimicking real drum machines with the human mouth and voice. Beatboxing performances can be done solo or with a group of singers, like in a cappella music. Due to their dependence on physical features (how the human mouth is shaped) and subjective interpretation (how a drum sound is translated to vocal percussion), beatboxing sounds tend to vary significantly across individual performers. However, most beatboxing sounds across performers can be loosely fit to standard drum sounds (e.g. kicks, snares, hi-hats, among others), especially by individuals that are familiar with these standard sounds. \n",
    "\n",
    "In this project, we aim to solve the problem of recognizing standard drum sounds used in beatboxing, with the most notable challenge being the variation across artists. \n",
    "\n",
    "###### Problem Statement:\n",
    "\n",
    "Given an audio recording of a solo beatboxing performance, design a tool that recognizes the distinct percussion sounds present in the recording (from a known dictionary of 10 percussion sounds) and identifies when the sounds occur in the recording.\n",
    "\n",
    "###### Sound Type Dictionary:\n",
    "Uses Standard Beatboxing Notation (SBN)\n",
    "1. Kick - B\n",
    "2. Snare 1 - Pf\n",
    "3. Snare 2 - Pch\n",
    "4. Snare 3 - K\n",
    "5. Snare 4 - ^Ksh\n",
    "6. Snare roll - rrh\n",
    "7. Closed hi-hat - t\n",
    "8. Open hi-hat - ts\n",
    "9. Rimshot - k\n",
    "10. Lip Oscillation - BB\n",
    "\n",
    "###### Final design:\n",
    "Input - audio query file (.wav)\n",
    "\n",
    "Output - soundDict, a Python dictionary with 10 keys corresponding to the 10 pre-defined percussion sounds, and values corresponding to time locations in the recording where the given sound type occured.\n",
    "\n",
    "We chose to implement a Hidden Markov Model with 10 *states* corresponding to each of the percussion sounds. The *observations* are each of the sounds made in the audio query (time-domain signals), and our *models* are multivariate normal distributions that are unique to each sound. \n",
    "\n",
    "\n",
    "###### Sections (outline of this notebook):\n",
    "1. Set up dependencies\n",
    "2. Observations functions - obtaining observations from audio data\n",
    "3. Model functions - training models based on ground truth data\n",
    "4. State functions - estimating states from observations and model \n",
    "3. Run - choose an individual audio file and run the beatboxing detection HMM to view resulting transcription\n",
    "3. Performance testing - F-measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dependencies\n",
    "Import statements, matplotlib and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa as lb\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import scipy.stats as ss\n",
    "import miscFuns as mf #python functions we wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "numDictSounds = 10\n",
    "sr = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAudioCalcSTFT(queryfile, sr=22050, hop_size=512, win_size=2048): #add in sample rate, hop size, window size stuff later\n",
    "    ''' TO DO\n",
    "    '''\n",
    "    y, sr = lb.core.load(queryfile, sr=sr)\n",
    "    S = lb.core.stft(y, n_fft=win_size, hop_length=hop_size)\n",
    "    Smag = np.abs(S)\n",
    "    return y, sr, Smag #we only use Smag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainObservations(audio, sr = 22050, backtrack=1000, model=False):\n",
    "    ''' TO DO\n",
    "    '''\n",
    "    \n",
    "    #get tempo\n",
    "    tempo_bpm = lb.beat.tempo(audio, sr=sr)\n",
    "    tempo_bpm = int(round(tempo_bpm[0]))\n",
    "    print(\"Tempo in bpm: \", tempo_bpm)\n",
    "    \n",
    "    #correct for rec1 error:\n",
    "    if tempo_bpm == 30:\n",
    "        print('Tempo miscalculated (=30), multiplying by 4')\n",
    "        tempo_bpm *= 4\n",
    "    \n",
    "    #determine eighth measure window size in samples\n",
    "    tempo_bps = tempo_bpm/60.0\n",
    "    quarterMeasure = 1/tempo_bps #time period for 1 beat, i.e. a quarter measure\n",
    "    eighthMeasure = quarterMeasure/2\n",
    "    win_size = int(round(eighthMeasure*sr)) #get window size in samples\n",
    "    print(\"8th measure window size:\", win_size)\n",
    "    \n",
    "    #set librosa peak picker parameters\n",
    "    premax = int((win_size/2.0)/512.0)\n",
    "    postmax = int((win_size/2.0)/512.0)\n",
    "    preavg = int((win_size/2.0)/512.0)\n",
    "    postavg = int((win_size/2.0)/512.0)\n",
    "    delta = 6.5\n",
    "    wait = int((win_size/2.0)/512.0)\n",
    "    \n",
    "    #compute onset envelope for audio and then use librosa peak picker to get beat locations\n",
    "    oenv = lb.onset.onset_strength(audio, sr=sr)\n",
    "    beats_frames = lb.util.peak_pick(oenv, pre_max=premax, post_max=postmax, pre_avg=preavg, post_avg=postavg, delta=delta, wait=wait)\n",
    "    \n",
    "    #convert to samples\n",
    "    beats = lb.frames_to_samples(beats_frames)\n",
    "    \n",
    "    #create observations array\n",
    "    if model == True: \n",
    "    #we hard-code size of obsvArray to be 40 for model training, because all our training data contains 40 known observations\n",
    "    #this would be addressed in future iterations\n",
    "        numObsv = 40 \n",
    "    else:\n",
    "        numObsv = numObsv = beats.shape[0]\n",
    "    obsvArray = np.zeros((win_size, numObsv))\n",
    "    for i in range(numObsv):\n",
    "        if i < beats.shape[0]:\n",
    "            onset = beats[i]\n",
    "        else:\n",
    "            onset = beats[-1] #imperfect solution to cases where we get <40 observations, we never hit this case if model==False\n",
    "        \n",
    "        #using backtrack parameter, obtain an observation from the input audio by taking a window of size win_size, starting at onset-backtrack\n",
    "        obsvArray[:,i] = audio[onset-backtrack:onset+win_size-backtrack]\n",
    "    \n",
    "    return tempo_bpm, beats, obsvArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFeatures(obsv, returnFVL=False): \n",
    "    '''single obsv: a 1D array representing samples over an eighth measure containing a percussion sound\n",
    "       returns a feature vector'''\n",
    "    fVecLen = 8\n",
    "    F = np.zeros(fVecLen)\n",
    "    stft = np.abs(lb.core.stft(obsv))\n",
    "    env = mf.envelope(obsv)\n",
    "    \n",
    "    #calculate feature parameters\n",
    "    maxInd = np.argmax(stft) # Linear/Flattened Index\n",
    "    maxInds = np.unravel_index(maxInd, stft.shape)\n",
    "    attack, decay, sustain, release = mf.calcADSR(obsv, env, sr=22050)\n",
    "    \n",
    "    F[0] = np.average(obsv) # May or may not keep\n",
    "    F[1] = maxInds[0] # Frequency (DFT index)\n",
    "    F[2] = maxInds[1] # Time (in frames)\n",
    "    F[3] = np.max(stft)   \n",
    "    F[4] = attack\n",
    "    F[5] = decay\n",
    "    F[6] = sustain\n",
    "    F[7] = release\n",
    "    \n",
    "    if returnFVL:\n",
    "        return F, fVecLen\n",
    "    else:\n",
    "        return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model functions\n",
    "Functions used to train the model - there are two types, a general model that includes all the training data, and an individual-specific model that uses the data for one person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGeneralModel(rec1_directory):\n",
    "    ''' input: \n",
    "        path to folder containing all rec1s\n",
    "    \n",
    "        internal variables:\n",
    "        numFiles - number of files in rec1 folder\n",
    "        fVec_accum - an array of size (fVecLen, 10) where fVec_accum[:,i] contains a running sum of all fVecs *known* to have state i (of 10 percussion sounds)\n",
    "        cov_accum - an array of size (fVecLen, 10, fVecLen) where cov_accum[:,i,:] contains a running sum of the outer product of \n",
    "        \n",
    "        output:\n",
    "        model - (transitions_matrix, means, covs)\n",
    "                transitions_matrix - np.ones((10,10))/100.0, equal transition probabilities\n",
    "                means - an array of size (fVecLen, 10) where means[:,i] specifies the mean feature vector for sound type i\n",
    "                covs - an array of size (fVecLen, 10, fVecLen) where cov[:,i,:] specifies the covariance matrix for sound type i\n",
    "        \n",
    "        pseudo-code:\n",
    "        iterate through each recording, and\n",
    "            load audio and obtain observations array\n",
    "            iterate through each observation, and \n",
    "                calculate feature vector for observation\n",
    "                add fVec to fVec_accum[:,i] where i is the *known* sound type for this observation (we know gt)\n",
    "                \n",
    "        divide fVec_accum by 4*numFiles to get means\n",
    "        \n",
    "        iterate through each recording again, and\n",
    "            load audio and obtain observations array\n",
    "            iterate through each observation, and \n",
    "                calculate feature vector for observation #redundancy can be fixed?\n",
    "                calculate diff = fVec - means[:,i] where i is the *known* sound type for this observation (we know gt)\n",
    "                add np.outer(diff, diff) with shape (fVecLen, fVecLen) to cov_accum[:,i,:]\n",
    "        \n",
    "        divide cov_accum by (4*numFiles) - 1, because of lower dof to get covs\n",
    "    '''\n",
    "    filenames = os.listdir(rec1_directory)\n",
    "    filenames.remove('.DS_Store')\n",
    "    \n",
    "    #parameters\n",
    "    numFiles = len(filenames)\n",
    "    fVecLen = 8\n",
    "    \n",
    "    #output data\n",
    "    means = np.zeros((fVecLen, numDictSounds))\n",
    "    covs = np.zeros((fVecLen, numDictSounds, fVecLen))\n",
    "    transition_matrix = np.ones((numDictSounds,numDictSounds))/(numDictSounds**2) #Equal probabilities to avoid bias\n",
    "    \n",
    "    #get fVec_accum\n",
    "    fVec_accum = np.zeros((fVecLen, numDictSounds))\n",
    "    print('Calculating means...')\n",
    "    for file in filenames:\n",
    "        print('File:', file)\n",
    "        #load audio and get observations+tempo\n",
    "        audio, sr, Smag = loadAudioCalcSTFT(rec1_directory+file)\n",
    "        tempo, beats, obsvArray = obtainObservations(audio, sr=22050, model=True) #tempo and observations need to be looked at, so that we're getting clear samples of each sound\n",
    "        numObsv = obsvArray.shape[1]\n",
    "        print('numObsv:', numObsv)\n",
    "        \n",
    "        #add breakpoint here if numObsv != 40, or if anything else is unexpected (e.g. tempo != 120)\n",
    "        #we need this for the rest of the code to work\n",
    "        \n",
    "        #iterate through observations\n",
    "        for obsv in range(numObsv):\n",
    "            (sNum, oNum) = divmod(obsv, 4) # Sound Number and observation number (4 observations of each sound)\n",
    "            thisFeatVec = calcFeatures(obsvArray[:,obsv])\n",
    "            fVec_accum[:,sNum] += thisFeatVec\n",
    "            \n",
    "    #calculate means\n",
    "    means = np.divide(fVec_accum,4*numFiles)\n",
    "    \n",
    "    #get cov_accum\n",
    "    cov_accum = np.zeros((fVecLen, numDictSounds, fVecLen))\n",
    "    print('Calculating covs...')\n",
    "    for file in filenames:\n",
    "        print('File:', file)\n",
    "        #load audio and get observations+tempo\n",
    "        audio, sr, Smag = loadAudioCalcSTFT(rec1_directory+file)\n",
    "        tempo, beats, obsvArray = obtainObservations(audio, sr=22050, model=True) #tempo and observations need to be looked at, so that we're getting clear samples of each sound\n",
    "        numObsv = obsvArray.shape[1]\n",
    "        print('numObsv:', numObsv)\n",
    "        \n",
    "        #add breakpoint here if numObsv != 40, or if anything else is unexpected (e.g. tempo != 120)\n",
    "        #we need this for the rest of the code to work\n",
    "        \n",
    "        #iterate through observations\n",
    "        for obsv in range(numObsv):\n",
    "            (sNum, oNum) = divmod(obsv, 4) # Sound Number and observation number (4 observations of each sound)\n",
    "            thisFeatVec = calcFeatures(obsvArray[:,obsv])\n",
    "            diff = thisFeatVec - means[:,sNum]\n",
    "            cov_accum[:,sNum,:] += np.outer(diff, diff)\n",
    "    \n",
    "    #calculate covs\n",
    "    covs = np.divide(cov_accum,(4*numFiles)-1)\n",
    "    \n",
    "    model = (transition_matrix, means, covs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModel(filename='audio_data/rsalazar_rec3.wav', gtlabels=None):\n",
    "    ''' Function for making model based on individual sound recordings, i.e. rec1\n",
    "        \n",
    "        NOTE: For ground truth labels to work, ONLY use this function with rec1.\n",
    "    '''\n",
    "    transition_matrix = np.ones((10,10))/100 # Equal probabilities to avoid bias\n",
    "    \n",
    "    audio, sr, Smag = loadAudioCalcSTFT(filename)\n",
    "    tempo, beats, obsvArray = obtainObservations(audio, model=True)\n",
    "    \n",
    "#     sNum = oNum = 0; \n",
    "    \n",
    "    # Loop through all observations\n",
    "    for obsv in np.arange(obsvArray.shape[1]):\n",
    "        (sNum, oNum) = divmod(obsv, 4) # Sound Number and observation number (4 observations of each sound)\n",
    "        \n",
    "        # Catch Errors\n",
    "        if sNum > numDictSounds-1:\n",
    "            print('Warning: Sound count higher than expected. Observations possibly flawed.')\n",
    "            break\n",
    "        \n",
    "        # Calculate feature vector of observation\n",
    "        thisFeatVec, fVecLen = calcFeatures(obsvArray[:,obsv], returnFVL=True)\n",
    "        \n",
    "        if obsv==0: # First observation of a sound \n",
    "            thisSound_fVecs = np.zeros((fVecLen,4)) # 4 feature vectors for 1 sound\n",
    "            thisSound_covs = np.zeros((fVecLen,4,fVecLen)) # 4 Covariance Matrices for each sound\n",
    "            fVecs_avg = np.zeros((fVecLen,numDictSounds)) # Initialize total means matrix\n",
    "            fVecs_cov = np.zeros((fVecLen,numDictSounds,fVecLen)) # Each covariance hass fvl x fvl dimensions\n",
    "        \n",
    "        # add feature vector to current group\n",
    "        thisSound_fVecs[:,oNum] = thisFeatVec\n",
    "        \n",
    "        # add Covariance matrix to current group\n",
    "        d1 = (thisFeatVec - fVecs_avg[:,sNum])\n",
    "        outer = np.outer(d1.reshape((fVecLen,1)), d1.reshape((1,fVecLen))) #don't need to reshape if we are using outer\n",
    "        thisSound_covs[:,oNum,:] = outer\n",
    "        \n",
    "        if oNum == 3: # Last observation of a sound\n",
    "            fVecs_avg[:,sNum] = np.mean(thisSound_fVecs, axis=1)\n",
    "            fVecs_cov[:,sNum,:] = (np.sum(thisSound_covs, axis=1))/3\n",
    "\n",
    "    return (transition_matrix, fVecs_avg, fVecs_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State functions\n",
    "Functions used to estimate the states, given observations and a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePairwiseSimilarityMatrix(F, means, covar):\n",
    "    '''\n",
    "    Compute a similarity matrix containing the conditional pdf value of a given feature vector given the percussion type.\n",
    "    \n",
    "    Arguments:\n",
    "    M is feature vector size\n",
    "    F -- feature matrix of size (M, N), where N is the number of audio frames\n",
    "    means -- a matrix of size (M, 10) whose i-th column specifies the mean chroma feature vector for\n",
    "        the ith percussion sound\n",
    "    covar -- matrix of size (M,10,M) specifying the estimated covariance matrix for all percussion sounds\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    S -- matrix of size (10, N) whose (i,j)-th element specifies the log of the conditional pdf value \n",
    "        of observing the j-th feature vector F[:,j] given that the percussion sound was of type i (following the order from our presentation)\n",
    "    '''\n",
    "    numObsv = F.shape[1]\n",
    "    numDictSounds = means.shape[1]\n",
    "    S = np.zeros((numDictSounds, numObsv))\n",
    "    \n",
    "    for i in range(numDictSounds):\n",
    "        cov = covar[:,i,:]\n",
    "        for j in range(numObsv):\n",
    "            S[i,j] = ss.multivariate_normal.logpdf(F[:,j], mean = means[:,i], cov = cov, allow_singular=True) #non-ideal\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBeatboxingRecognitionHMM(beats, obsvArray, model):\n",
    "    '''\n",
    "    Estimate the beatboxing sound given an array of sounds\n",
    "    \n",
    "    Arguments:\n",
    "    obsvArray -- array of shape (m,n) where m is the 8th measure window size and n is the number of observations in the audio recording\n",
    "    model -- trained hidden markov model\n",
    "    \n",
    "    Returns:\n",
    "    soundDict -- dictionary where the keys are the ten pre-defined beatboxing sounds, and the values are lists of the \n",
    "    location(s) of the sound in the query file\n",
    "    this can be used to make transcriptions of the beatboxing recording\n",
    "    '''\n",
    "    soundDict = {}\n",
    "\n",
    "    numObsv = obsvArray.shape[1]\n",
    "    (transition_matrix, means, covar) = model\n",
    "    \n",
    "    numFeats = 8\n",
    "    F = np.zeros((numFeats,numObsv))\n",
    "    for obsv in range(numObsv):\n",
    "        F[:,obsv] = calcFeatures(obsvArray[:,obsv])\n",
    "\n",
    "    S = generatePairwiseSimilarityMatrix(F, means, covar) \n",
    "    \n",
    "    beat_transcription = []\n",
    "    \n",
    "    for obsv in range(numObsv):\n",
    "        beat_transcription += [(mf.num2Sound[np.argmax(S[:,obsv])], beats[obsv]/np.float64(sr))]\n",
    "    \n",
    "    return beat_transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "Choose an audio file from our dataset and run the beatboxing detection HMM to view results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#use this to set queryfile from data and listen to audio\n",
    "queryfile = 'audio_data/rsalazar_rec6.wav'\n",
    "ipd.Audio(queryfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get observations from audio\n",
    "audio, sr, Smag = loadAudioCalcSTFT(queryfile)\n",
    "tempo, beats, obsvArray = obtainObservations(audio)\n",
    "print(tempo)\n",
    "print(beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization, playing around with data\n",
    "print(\"Number of observations: \", obsvArray.shape[1])\n",
    "plt.figure(1)\n",
    "plt.plot(audio)\n",
    "plt.title('Complete beatboxing performance')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n",
    "#set observation index\n",
    "obsv_idx = 9\n",
    "plt.figure(2)\n",
    "plt.plot(obsvArray[:,obsv_idx])\n",
    "plt.title('Single observation')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(3)\n",
    "# plt.imshow(Smag, origin = 'lower', cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train general model - this takes about 2 minutes\n",
    "model = makeGeneralModel('training_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the HMM\n",
    "output = runBeatboxingRecognitionHMM(beats, obsvArray, model)#run HMM and get sound dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "Use F-measure to characterize our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
