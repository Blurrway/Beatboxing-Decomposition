{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beatboxing Decomposition: Testing & Performance Characterization\n",
    "## *Version 1.0 (December 2018)*\n",
    "\n",
    "This file documents the performance of the system in its first iteration; i.e., the version that we were able to present as a Final Project for Music Signals Processing (E190AP) at Harvey Mudd College in Fall 2018. Although there were additions to the code after the semester ended, these changes were mainly comprised of extra data, convenience functions/files (such as allowing models to be stored in csv files), and minor cosmetic changes. The actual processing and features of the program remain unchanged from the Final Project version.\n",
    "\n",
    "### Methodology\n",
    "This program uses a Hidden Markov Model (HMM) to make its sound estimations/predictions. In brief, HMMs are based off of the concept of states and observations. For some sequence of observations, we assume there is some underlying sequence of states that produces these observations. In order to guess these states, we can use their observations (probability that observation A came from state X) and the estimated previous state (probability that state X follows state Y). These probabilities are called emission probabilities and transition probabilities, respectively. The model is comprised of these probabilities, and it uses the combination of them to estimate the state for a given observation.\n",
    "\n",
    "Our program in particular uses a simplified HMM that ignores the transition probabilities. Fundamentally, our \"states\" are the name of each sound, and our \"observations\" are the audio from each sound; however, to reduce the amount of data the program would have to handle, we condensed each sound into feature vectors. These consisted of 8 measures that are loosely representative of the particular sound:\n",
    "\n",
    "- Average energy\n",
    "- Maximum energy\n",
    "- Frequency of max energy (*DFT index*)\n",
    "- Time of max energy (*seconds from start of sound*)\n",
    "- Attack (*time from onset to max energy*)\n",
    "- Decay (*time from max energy to steady state*)\n",
    "- Sustain (*duration of steady state*)\n",
    "- Release (*time from end of steady state to end of sound*)\n",
    "\n",
    "We then used these feature vectors as our ovservations. During training, the model calculates the feature vectors for each observation and builds a Gaussian distribution (defined by mean and covariance) that is associated with the sound label. Note that we did NOT proceed to make a transitions matrix based on the observed state transitions. We didn't want to influence our early results with a transitions matrix mainly because of our small sample size of true freestyles. At this point, transition probabilities would likely be overly biased towards the sample beat patterns that we created, rather than informed from a general knowledge of what beatboxers typically do. Thus, the current results are completely formed by the means and covariances (i.e., the emission probabilities).\n",
    "\n",
    "### System Use\n",
    "The current system can use two different types of models: general and individual. The general model is constructed based on recordings from 6 different beatboxers. This model attempts to encompass general sound characteristics found across a wide range of beatboxers by averaging the mean features from each beatboxer's recording of the individual sounds. In short, the general model is a **pre-set model** that aims to identify sounds made by **any given beatboxer**. On the other hand, individual models are **user-created models** that aim to identify sounds made by **one particular beatboxer**. By feeding the program individual sounds recorded by one particular beatboxer, users can train a personal model fine-tuned to recognize sounds from that particular person.\n",
    "\n",
    "Note that the differences between the creation and intended use of the general and individual models also lead to very different runtime concerns. For the general model, we are *NOT* concerned with how long it takes to create the model. The general model is created and saved beforehand by the developers, and users are not expected to re-create the general model from recordings. However, individual models are user-created, which makes short runtime for model creation a more pressing concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Project File\n",
    "One change that *was* made after the end of the semester was the separation of the project into Python files (for end use as modules) and Jupyter Notebooks (for experimentation and documentation). This allows us to import the entire project in one line (assuming you don't need access to internal helper functions in the `miscFuns.py` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Beatboxing_Detection import *\n",
    "import miscFuns as mf # Optional, only needed if you want to test/use individual helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other modules used in this document\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-constructing the General Model\n",
    "The different components of the general model are saved in csv files. However, this is not a usable data format for the model. Thus, we need to first create a model from the csv files. This process is wrapped into a quick and easy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = csv2model() # Default arguments set to proper filenames of csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gen_model` variable now has the mean feature vectors and covariances of the general model. It also has a transitions matrix (probabilities that sound X comes after sound Y), but we aren't using that in this iteration of the system (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Individual Model\n",
    "In order to create an individual model, we can use the `makeIndividualModel` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo in bpm:  123\n",
      "8th measure window size: 5378\n"
     ]
    }
   ],
   "source": [
    "person = 'scozier' # Name recorded in filename of recording\n",
    "scozier_model = makeIndividualModel(person) # Uses scozier's rec1 to make an individual model\n",
    "model2csv(scozier_model, title='scozier') # Save individual model to csv file for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scozier_model` variable holds the same type of information (means, covariances, and transition probabilities) as the `gen_model` variable, but it was generated from one recording rather than from multiple recordings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Output\n",
    "The system has two kinds of outputs. For individual recordings, you can retrieve the list of sounds and the times at which they occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo in bpm:  161\n",
      "8th measure window size: 4109\n"
     ]
    }
   ],
   "source": [
    "queryfile = 'audio_data/scozier_rec2.wav'\n",
    "audio, sr, Smag = loadAudioCalcSTFT(queryfile)\n",
    "tempo, beats, obsvArray = obtainObservations(audio)\n",
    "transcription = runBeatboxingRecognitionHMM(beats, obsvArray, gen_model) #run HMM and get sound list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Pf', 0.2089795918367347),\n",
       " ('ts', 0.5804988662131519),\n",
       " ('^Ksh', 0.9520181405895691),\n",
       " ('ts', 1.253877551020408),\n",
       " ('t', 1.6718367346938776),\n",
       " ('t', 2.043356009070295),\n",
       " ('B', 2.391655328798186),\n",
       " ('t', 2.7631746031746034),\n",
       " ('t', 3.1579138321995464),\n",
       " ('^Ksh', 3.5294331065759637),\n",
       " ('^Ksh', 3.900952380952381),\n",
       " ('t', 4.272471655328798),\n",
       " ('t', 4.6439909297052155),\n",
       " ('^Ksh', 4.992290249433107),\n",
       " ('^Ksh', 5.38702947845805),\n",
       " ('t', 5.758548752834467),\n",
       " ('^Ksh', 6.130068027210885),\n",
       " ('rrh', 6.548027210884354),\n",
       " ('^Ksh', 6.919546485260771),\n",
       " ('t', 7.267845804988662),\n",
       " ('t', 7.63936507936508),\n",
       " ('^Ksh', 8.010884353741497),\n",
       " ('B', 8.359183673469389),\n",
       " ('ts', 8.730702947845804),\n",
       " ('t', 9.125442176870749),\n",
       " ('Pf', 9.496961451247165),\n",
       " ('^Ksh', 9.868480725623582),\n",
       " ('t', 10.24),\n",
       " ('t', 10.634739229024943),\n",
       " ('Pf', 10.95981859410431),\n",
       " ('Pch', 11.354557823129252),\n",
       " ('ts', 11.749297052154194)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(transcription)) # Number of sounds that the program recognized\n",
    "transcription # List of (sound, time) tuples, where time is in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple recordings, you can retrieve summary statistics on how well the estimated sequence of sounds matched up with the ground truths. Note that this evaluates the accuracy of the *order* of sounds, not the exact time of each sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sound match percentages (absGTRes, freeTempoRes):\n",
      "[[0.8125, 0.6875, 0.25, 0.875, 0.20689655172413793, 0.375], [0.061224489795918366, 0.18181818181818182, 0.0, 0.16326530612244897, 0.10416666666666667, 0.022222222222222223]]\n",
      "Absolute Ground Truth Stats for query_data, recording(s) [2, 8, 9]:\n",
      "DescribeResult(nobs=6, minmax=(0.20689655172413793, 0.875), mean=0.5344827586206896, variance=0.0860675535077289, skewness=0.020143304179259944, kurtosis=-1.746802194891731)\n",
      "\n",
      "Free Tempo Stats for query_data, recording(s) [2, 8, 9]:\n",
      "DescribeResult(nobs=6, minmax=(0.0, 0.18181818181818182), mean=0.08878281110423968, variance=0.005502409178641712, skewness=0.11083517210497144, kurtosis=-1.5090389122580075)\n",
      "\n",
      "Multiple Choice Ground Truth Results for query_data, recording 9:\n",
      "Predicted patterns: [0, 0, 2, 0, 0, 1]\n",
      "Ground Truth patterns: [0, 0, 2, 0, 0, 1]\n",
      "Match rate: 100.0%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multChoiceRes, freeTempoRes, absGTRes = batchBeatboxingRecognition(recNums=[2,8,9], model=gen_model, disp='res')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the results are broken down into three categories: absolute ground truth results, free tempo results, and multiple choice results. The absolute ground truth results show how well the system transcribes the order of sounds when the recording’s tempo was fixed. The free tempo results are similar to the absolute ground truth results; however, the ground truth is constructed based on the computer-estimated tempo, rather than a known tempo. Both of these types of results are given as match rates, which are the percentage of sounds predicted correctly for a particular recording. \n",
    "\n",
    "The multiple choice results are slightly different. The goal of this part was to test if the program could tell the difference between three different beat patterns. The program analyzes each recording against the different ground truths and selects the pattern that best matches the recording. As shown in the results above, the system appears to perform this task well (at least for this particular set of patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "-------------\n",
    "--------------\n",
    "------------\n",
    "--------------\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(beat2) # SBN representation\n",
    "# beat1_gt = mf.sbn2gtlabels(beat1, 80, offset=0.2, listOut=True)\n",
    "# print(len(beat1_gt))\n",
    "# print(beat1_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = time.time()\n",
    "# queryfile = 'audio_data/scozier_rec2.wav'; t2 = time.time()\n",
    "# audio, sr, Smag = loadAudioCalcSTFT(queryfile); t3 = time.time()\n",
    "# tempo, beats, obsvArray = obtainObservations(audio); t4 = time.time()\n",
    "# transcription = runBeatboxingRecognitionHMM(beats, obsvArray, gen_model) #run HMM and get sound list\n",
    "# t5 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Total time elapsed: {} s'.format(t5-t1))\n",
    "# print('  loadAudioCalcSTFT:           {} s'.format(t3-t2))\n",
    "# print('  obtainObservations:          {} s'.format(t4-t3))\n",
    "# print('  runBeatboxingRecognitionHMM: {} s'.format(t5-t4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
